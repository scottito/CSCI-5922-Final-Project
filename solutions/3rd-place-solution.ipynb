{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e7c8b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T16:56:10.865835Z",
     "iopub.status.busy": "2025-03-13T16:56:10.865564Z",
     "iopub.status.idle": "2025-03-13T16:56:43.786597Z",
     "shell.execute_reply": "2025-03-13T16:56:43.785574Z"
    },
    "papermill": {
     "duration": 32.927531,
     "end_time": "2025-03-13T16:56:43.788327",
     "exception": false,
     "start_time": "2025-03-13T16:56:10.860796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rtdl_num_embeddings -q --no-index --find-links=/kaggle/input/cibmtr-competition/rtdl_num_embeddings\n",
    "!pip install -q /kaggle/input/cibmtr-competition/qhoptim-1.1.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/pytorch_lightning-2.4.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/torchmetrics-1.5.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/pytorch_tabnet-4.1.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/einops-0.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/cibmtr-competition/pytorch_tabular-1.1.1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3617814d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:56:43.796164Z",
     "iopub.status.busy": "2025-03-13T16:56:43.795869Z",
     "iopub.status.idle": "2025-03-13T16:56:57.441655Z",
     "shell.execute_reply": "2025-03-13T16:56:57.440958Z"
    },
    "papermill": {
     "duration": 13.651107,
     "end_time": "2025-03-13T16:56:57.443229",
     "exception": false,
     "start_time": "2025-03-13T16:56:43.792122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from qhoptim.pyt import QHAdam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "import rtdl_num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1ca478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:56:57.451234Z",
     "iopub.status.busy": "2025-03-13T16:56:57.450975Z",
     "iopub.status.idle": "2025-03-13T16:56:57.705539Z",
     "shell.execute_reply": "2025-03-13T16:56:57.704786Z"
    },
    "papermill": {
     "duration": 0.260123,
     "end_time": "2025-03-13T16:56:57.706933",
     "exception": false,
     "start_time": "2025-03-13T16:56:57.446810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# License: https://github.com/yandex-research/tabm/blob/main/LICENSE\n",
    "\n",
    "# NOTE\n",
    "# The minimum required versions of the dependencies are specified in README.md.\n",
    "\n",
    "import itertools\n",
    "from typing import Any, Literal, Union\n",
    "\n",
    "import rtdl_num_embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Initialization\n",
    "# ======================================================================================\n",
    "def init_rsqrt_uniform_(x: Tensor, d: int) -> Tensor:\n",
    "    assert d > 0\n",
    "    d_rsqrt = d**-0.5\n",
    "    return nn.init.uniform_(x, -d_rsqrt, d_rsqrt)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def init_random_signs_(x: Tensor) -> Tensor:\n",
    "    return x.bernoulli_(0.5).mul_(2).add_(-1)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Modules\n",
    "# ======================================================================================\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"N linear layers applied in parallel to N disjoint parts of the input.\n",
    "\n",
    "    **Shape**\n",
    "\n",
    "    - Input: ``(B, N, in_features)``\n",
    "    - Output: ``(B, N, out_features)``\n",
    "\n",
    "    The i-th linear layer is applied to the i-th matrix of the shape (B, in_features).\n",
    "\n",
    "    Technically, this is a simplified version of delu.nn.NLinear:\n",
    "    https://yura52.github.io/delu/stable/api/generated/delu.nn.NLinear.html.\n",
    "    The difference is that this layer supports only 3D inputs\n",
    "    with exactly one batch dimension. By contrast, delu.nn.NLinear supports\n",
    "    any number of batch dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n: int, in_features: int, out_features: int, bias: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(n, in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.empty(n, out_features)) if bias else None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        d = self.weight.shape[-2]\n",
    "        init_rsqrt_uniform_(self.weight, d)\n",
    "        if self.bias is not None:\n",
    "            init_rsqrt_uniform_(self.bias, d)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.ndim == 3\n",
    "        assert x.shape[-(self.weight.ndim - 1) :] == self.weight.shape[:-1]\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x @ self.weight\n",
    "        x = x.transpose(0, 1)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class OneHotEncoding0d(nn.Module):\n",
    "    # Input:  (*, n_cat_features=len(cardinalities))\n",
    "    # Output: (*, sum(cardinalities))\n",
    "\n",
    "    def __init__(self, cardinalities: list[int]) -> None:\n",
    "        super().__init__()\n",
    "        self._cardinalities = cardinalities\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert x.ndim >= 1\n",
    "        assert x.shape[-1] == len(self._cardinalities)\n",
    "\n",
    "        return torch.cat(\n",
    "            [\n",
    "                # NOTE\n",
    "                # This is a quick hack to support out-of-vocabulary categories.\n",
    "                #\n",
    "                # Recall that lib.data.transform_cat encodes categorical features\n",
    "                # as follows:\n",
    "                # - In-vocabulary values receive indices from `range(cardinality)`.\n",
    "                # - All out-of-vocabulary values (i.e. new categories in validation\n",
    "                #   and test data that are not presented in the training data)\n",
    "                #   receive the index `cardinality`.\n",
    "                #\n",
    "                # As such, the line below will produce the standard one-hot encoding for\n",
    "                # known categories, and the all-zeros encoding for unknown categories.\n",
    "                # This may not be the best approach to deal with unknown values,\n",
    "                # but should be enough for our purposes.\n",
    "                nn.functional.one_hot(x[..., i], cardinality + 1)[..., :-1]\n",
    "                for i, cardinality in enumerate(self._cardinalities)\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ScaleEnsemble(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        d: int,\n",
    "        *,\n",
    "        init: Literal['ones', 'normal', 'random-signs'],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(k, d))\n",
    "        self._weight_init = init\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        if self._weight_init == 'ones':\n",
    "            nn.init.ones_(self.weight)\n",
    "        elif self._weight_init == 'normal':\n",
    "            nn.init.normal_(self.weight)\n",
    "        elif self._weight_init == 'random-signs':\n",
    "            init_random_signs_(self.weight)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown weight_init: {self._weight_init}')\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        assert x.ndim >= 2\n",
    "        return x * self.weight\n",
    "\n",
    "\n",
    "class LinearEfficientEnsemble(nn.Module):\n",
    "    \"\"\"\n",
    "    This layer is a more configurable version of the \"BatchEnsemble\" layer\n",
    "    from the paper\n",
    "    \"BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning\"\n",
    "    (link: https://arxiv.org/abs/2002.06715).\n",
    "\n",
    "    First, this layer allows to select only some of the \"ensembled\" parts:\n",
    "    - the input scaling  (r_i in the BatchEnsemble paper)\n",
    "    - the output scaling (s_i in the BatchEnsemble paper)\n",
    "    - the output bias    (not mentioned in the BatchEnsemble paper,\n",
    "                          but is presented in public implementations)\n",
    "\n",
    "    Second, the initialization of the scaling weights is configurable\n",
    "    through the `scaling_init` argument.\n",
    "\n",
    "    NOTE\n",
    "    The term \"adapter\" is used in the TabM paper only to tell the story.\n",
    "    The original BatchEnsemble paper does NOT use this term. So this class also\n",
    "    avoids the term \"adapter\".\n",
    "    \"\"\"\n",
    "\n",
    "    r: Union[None, Tensor]\n",
    "    s: Union[None, Tensor]\n",
    "    bias: Union[None, Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        *,\n",
    "        k: int,\n",
    "        ensemble_scaling_in: bool,\n",
    "        ensemble_scaling_out: bool,\n",
    "        ensemble_bias: bool,\n",
    "        scaling_init: Literal['ones', 'random-signs'],\n",
    "    ):\n",
    "        assert k > 0\n",
    "        if ensemble_bias:\n",
    "            assert bias\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_parameter(\n",
    "            'r',\n",
    "            (\n",
    "                nn.Parameter(torch.empty(k, in_features))\n",
    "                if ensemble_scaling_in\n",
    "                else None\n",
    "            ),  # type: ignore[code]\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            's',\n",
    "            (\n",
    "                nn.Parameter(torch.empty(k, out_features))\n",
    "                if ensemble_scaling_out\n",
    "                else None\n",
    "            ),  # type: ignore[code]\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            'bias',\n",
    "            (\n",
    "                nn.Parameter(torch.empty(out_features))  # type: ignore[code]\n",
    "                if bias and not ensemble_bias\n",
    "                else nn.Parameter(torch.empty(k, out_features))\n",
    "                if ensemble_bias\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.k = k\n",
    "        self.scaling_init = scaling_init\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_rsqrt_uniform_(self.weight, self.in_features)\n",
    "        scaling_init_fn = {'ones': nn.init.ones_, 'random-signs': init_random_signs_}[\n",
    "            self.scaling_init\n",
    "        ]\n",
    "        if self.r is not None:\n",
    "            scaling_init_fn(self.r)\n",
    "        if self.s is not None:\n",
    "            scaling_init_fn(self.s)\n",
    "        if self.bias is not None:\n",
    "            bias_init = torch.empty(\n",
    "                # NOTE: the shape of bias_init is (out_features,) not (k, out_features).\n",
    "                # It means that all biases have the same initialization.\n",
    "                # This is similar to having one shared bias plus\n",
    "                # k zero-initialized non-shared biases.\n",
    "                self.out_features,\n",
    "                dtype=self.weight.dtype,\n",
    "                device=self.weight.device,\n",
    "            )\n",
    "            bias_init = init_rsqrt_uniform_(bias_init, self.in_features)\n",
    "            with torch.inference_mode():\n",
    "                self.bias.copy_(bias_init)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x.shape == (B, K, D)\n",
    "        assert x.ndim == 3\n",
    "\n",
    "        # >>> The equation (5) from the BatchEnsemble paper (arXiv v2).\n",
    "        if self.r is not None:\n",
    "            x = x * self.r\n",
    "        x = x @ self.weight.T\n",
    "        if self.s is not None:\n",
    "            x = x * self.s\n",
    "        # <<<\n",
    "\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_in: Union[None, int] = None,\n",
    "        d_out: Union[None, int] = None,\n",
    "        n_blocks: int,\n",
    "        d_block: int,\n",
    "        dropout: float,\n",
    "        activation: str = 'ReLU',\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        d_first = d_block if d_in is None else d_in\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(d_first if i == 0 else d_block, d_block),\n",
    "                    getattr(nn, activation)(),\n",
    "                    nn.Dropout(dropout),\n",
    "                )\n",
    "                for i in range(n_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.output = None if d_out is None else nn.Linear(d_block, d_out)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        if self.output is not None:\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_efficient_ensemble(module: nn.Module, **kwargs) -> None:\n",
    "    \"\"\"Replace torch.nn.Linear modules with LinearEfficientEnsemble.\n",
    "\n",
    "    NOTE\n",
    "    In the paper, there are no experiments with networks with normalization layers.\n",
    "    Perhaps, their trainable weights (the affine transformations) also need\n",
    "    \"ensemblification\" as in the paper about \"FiLM-Ensemble\".\n",
    "    Additional experiments are required to make conclusions.\n",
    "    \"\"\"\n",
    "    for name, submodule in list(module.named_children()):\n",
    "        if isinstance(submodule, nn.Linear):\n",
    "            module.add_module(\n",
    "                name,\n",
    "                LinearEfficientEnsemble(\n",
    "                    in_features=submodule.in_features,\n",
    "                    out_features=submodule.out_features,\n",
    "                    bias=submodule.bias is not None,\n",
    "                    **kwargs,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            make_efficient_ensemble(submodule, **kwargs)\n",
    "\n",
    "\n",
    "def _get_first_ensemble_layer(backbone: MLP) -> LinearEfficientEnsemble:\n",
    "    if isinstance(backbone, MLP):\n",
    "        return backbone.blocks[0][0]  # type: ignore[code]\n",
    "    else:\n",
    "        raise RuntimeError(f'Unsupported backbone: {backbone}')\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _init_first_adapter(\n",
    "    weight: Tensor,\n",
    "    distribution: Literal['normal', 'random-signs'],\n",
    "    init_sections: list[int],\n",
    ") -> None:\n",
    "    \"\"\"Initialize the first adapter.\n",
    "\n",
    "    NOTE\n",
    "    The `init_sections` argument is a historical artifact that accidentally leaked\n",
    "    from irrelevant experiments to the final models. Perhaps, the code related\n",
    "    to `init_sections` can be simply removed, but this was not tested.\n",
    "    \"\"\"\n",
    "    assert weight.ndim == 2\n",
    "    assert weight.shape[1] == sum(init_sections)\n",
    "\n",
    "    if distribution == 'normal':\n",
    "        init_fn_ = nn.init.normal_\n",
    "    elif distribution == 'random-signs':\n",
    "        init_fn_ = init_random_signs_\n",
    "    else:\n",
    "        raise ValueError(f'Unknown distribution: {distribution}')\n",
    "\n",
    "    section_bounds = [0, *torch.tensor(init_sections).cumsum(0).tolist()]\n",
    "    for i in range(len(init_sections)):\n",
    "        # NOTE\n",
    "        # As noted above, this section-based initialization is an arbitrary historical\n",
    "        # artifact. Consider the first adapter of one ensemble member.\n",
    "        # This adapter vector is implicitly split into \"sections\",\n",
    "        # where one section corresponds to one feature. The code below ensures that\n",
    "        # the adapter weights in one section are initialized with the same random value\n",
    "        # from the given distribution.\n",
    "        w = torch.empty((len(weight), 1), dtype=weight.dtype, device=weight.device)\n",
    "        init_fn_(w)\n",
    "        weight[:, section_bounds[i] : section_bounds[i + 1]] = w\n",
    "\n",
    "\n",
    "_CUSTOM_MODULES = {\n",
    "    # https://docs.python.org/3/library/stdtypes.html#definition.__name__\n",
    "    CustomModule.__name__: CustomModule\n",
    "    for CustomModule in [\n",
    "        rtdl_num_embeddings.LinearEmbeddings,\n",
    "        rtdl_num_embeddings.LinearReLUEmbeddings,\n",
    "        rtdl_num_embeddings.PeriodicEmbeddings,\n",
    "        rtdl_num_embeddings.PiecewiseLinearEmbeddings,\n",
    "        MLP,\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def make_module(type: str, *args, **kwargs) -> nn.Module:\n",
    "    Module = getattr(nn, type, None)\n",
    "    if Module is None:\n",
    "        Module = _CUSTOM_MODULES[type]\n",
    "    return Module(*args, **kwargs)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Optimization\n",
    "# ======================================================================================\n",
    "def default_zero_weight_decay_condition(\n",
    "    module_name: str, module: nn.Module, parameter_name: str, parameter: nn.Parameter\n",
    "):\n",
    "    from rtdl_num_embeddings import _Periodic\n",
    "\n",
    "    del module_name, parameter\n",
    "    return parameter_name.endswith('bias') or isinstance(\n",
    "        module,\n",
    "        nn.BatchNorm1d\n",
    "        or nn.LayerNorm\n",
    "        or nn.InstanceNorm1d\n",
    "        or rtdl_num_embeddings.LinearEmbeddings\n",
    "        or rtdl_num_embeddings.LinearReLUEmbeddings\n",
    "        or _Periodic,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_parameter_groups(\n",
    "    module: nn.Module,\n",
    "    zero_weight_decay_condition=default_zero_weight_decay_condition,\n",
    "    custom_groups: Union[None, list[dict[str, Any]]] = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    if custom_groups is None:\n",
    "        custom_groups = []\n",
    "    custom_params = frozenset(\n",
    "        itertools.chain.from_iterable(group['params'] for group in custom_groups)\n",
    "    )\n",
    "    assert len(custom_params) == sum(\n",
    "        len(group['params']) for group in custom_groups\n",
    "    ), 'Parameters in custom_groups must not intersect'\n",
    "    zero_wd_params = frozenset(\n",
    "        p\n",
    "        for mn, m in module.named_modules()\n",
    "        for pn, p in m.named_parameters()\n",
    "        if p not in custom_params and zero_weight_decay_condition(mn, m, pn, p)\n",
    "    )\n",
    "    default_group = {\n",
    "        'params': [\n",
    "            p\n",
    "            for p in module.parameters()\n",
    "            if p not in custom_params and p not in zero_wd_params\n",
    "        ]\n",
    "    }\n",
    "    return [\n",
    "        default_group,\n",
    "        {'params': list(zero_wd_params), 'weight_decay': 0.0},\n",
    "        *custom_groups,\n",
    "    ]\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# The model\n",
    "# ======================================================================================\n",
    "class Model(nn.Module):\n",
    "    \"\"\"MLP & TabM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_num_features: int,\n",
    "        cat_cardinalities: list[int],\n",
    "        n_classes: Union[None, int],\n",
    "        backbone: dict,\n",
    "        bins: Union[None, list[Tensor]],  # For piecewise-linear encoding/embeddings.\n",
    "        num_embeddings: Union[None, dict] = None,\n",
    "        arch_type: Literal[\n",
    "            # Plain feed-forward network without any kind of ensembling.\n",
    "            'plain',\n",
    "            #\n",
    "            # TabM-mini\n",
    "            'tabm-mini',\n",
    "            #\n",
    "            # TabM-mini. The first adapter is initialized from the normal distribution.\n",
    "            # This is used in Section 5.1 of the paper.\n",
    "            'tabm-mini-normal',\n",
    "            #\n",
    "            # TabM\n",
    "            'tabm',\n",
    "            #\n",
    "            # TabM. The first adapter is initialized from the normal distribution.\n",
    "            # This variation is not used in the paper, but there is a preliminary\n",
    "            # evidence that may be a better default strategy.\n",
    "            'tabm-normal',\n",
    "        ],\n",
    "        k: Union[None, int] = None,\n",
    "    ) -> None:\n",
    "        # >>> Validate arguments.\n",
    "        assert n_num_features >= 0\n",
    "        assert n_num_features or cat_cardinalities\n",
    "        if arch_type == 'plain':\n",
    "            assert k is None\n",
    "        else:\n",
    "            assert k is not None\n",
    "            assert k > 0\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # >>> Continuous (numerical) features\n",
    "        first_adapter_sections = []  # See the comment in `_init_first_adapter`.\n",
    "\n",
    "        if n_num_features == 0:\n",
    "            assert bins is None\n",
    "            self.num_module = None\n",
    "            d_num = 0\n",
    "\n",
    "        elif num_embeddings is None:\n",
    "            assert bins is None\n",
    "            self.num_module = None\n",
    "            d_num = n_num_features\n",
    "            first_adapter_sections.extend(1 for _ in range(n_num_features))\n",
    "\n",
    "        else:\n",
    "            if bins is None:\n",
    "                self.num_module = make_module(\n",
    "                    **num_embeddings, n_features=n_num_features\n",
    "                )\n",
    "            else:\n",
    "                assert num_embeddings['type'].startswith('PiecewiseLinearEmbeddings')\n",
    "                self.num_module = make_module(**num_embeddings, bins=bins)\n",
    "            d_num = n_num_features * num_embeddings['d_embedding']\n",
    "            first_adapter_sections.extend(\n",
    "                num_embeddings['d_embedding'] for _ in range(n_num_features)\n",
    "            )\n",
    "\n",
    "        # >>> Categorical features\n",
    "        self.cat_module = (\n",
    "            OneHotEncoding0d(cat_cardinalities) if cat_cardinalities else None\n",
    "        )\n",
    "        first_adapter_sections.extend(cat_cardinalities)\n",
    "        d_cat = sum(cat_cardinalities)\n",
    "\n",
    "        # >>> Backbone\n",
    "        d_flat = d_num + d_cat\n",
    "        self.minimal_ensemble_adapter = None\n",
    "        # Any backbone can be here but we provide only MLP\n",
    "        self.backbone = make_module(d_in=d_flat, **backbone)\n",
    "\n",
    "        if arch_type != 'plain':\n",
    "            assert k is not None\n",
    "            first_adapter_init = (\n",
    "                'normal'\n",
    "                if arch_type in ('tabm-mini-normal', 'tabm-normal')\n",
    "                # For other arch_types, the initialization depends\n",
    "                # on the presense of num_embeddings.\n",
    "                else 'random-signs'\n",
    "                if num_embeddings is None\n",
    "                else 'normal'\n",
    "            )\n",
    "\n",
    "            if arch_type in ('tabm-mini', 'tabm-mini-normal'):\n",
    "                # Minimal ensemble\n",
    "                self.minimal_ensemble_adapter = ScaleEnsemble(\n",
    "                    k,\n",
    "                    d_flat,\n",
    "                    init='random-signs' if num_embeddings is None else 'normal',\n",
    "                )\n",
    "                _init_first_adapter(\n",
    "                    self.minimal_ensemble_adapter.weight,  # type: ignore[code]\n",
    "                    first_adapter_init,\n",
    "                    first_adapter_sections,\n",
    "                )\n",
    "\n",
    "            elif arch_type in ('tabm', 'tabm-normal'):\n",
    "                # Like BatchEnsemble, but all multiplicative adapters,\n",
    "                # except for the very first one, are initialized with ones.\n",
    "                make_efficient_ensemble(\n",
    "                    self.backbone,\n",
    "                    k=k,\n",
    "                    ensemble_scaling_in=True,\n",
    "                    ensemble_scaling_out=True,\n",
    "                    ensemble_bias=True,\n",
    "                    scaling_init='ones',\n",
    "                )\n",
    "                _init_first_adapter(\n",
    "                    _get_first_ensemble_layer(self.backbone).r,  # type: ignore[code]\n",
    "                    first_adapter_init,\n",
    "                    first_adapter_sections,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Unknown arch_type: {arch_type}')\n",
    "\n",
    "        # >>> Output\n",
    "        d_block = backbone['d_block']\n",
    "        d_out = 1 if n_classes is None else n_classes\n",
    "        self.output = (\n",
    "            nn.Linear(d_block, d_out)\n",
    "            if arch_type == 'plain'\n",
    "            else NLinear(k, d_block, d_out)  # type: ignore[code]\n",
    "        )\n",
    "\n",
    "        # >>>\n",
    "        self.arch_type = arch_type\n",
    "        self.k = k\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_num: Union[None, Tensor] = None,\n",
    "        x_cat: Union[None, Tensor] = None\n",
    "    ) -> Tensor:\n",
    "        x = []\n",
    "        if x_num is not None:\n",
    "            x.append(x_num if self.num_module is None else self.num_module(x_num))\n",
    "        if x_cat is None:\n",
    "            assert self.cat_module is None\n",
    "        else:\n",
    "            assert self.cat_module is not None\n",
    "            x.append(self.cat_module(x_cat).float())\n",
    "        x = torch.column_stack([x_.flatten(1, -1) for x_ in x])\n",
    "\n",
    "        if self.k is not None:\n",
    "            x = x[:, None].expand(-1, self.k, -1)  # (B, D) -> (B, K, D)\n",
    "            if self.minimal_ensemble_adapter is not None:\n",
    "                x = self.minimal_ensemble_adapter(x)\n",
    "        else:\n",
    "            assert self.minimal_ensemble_adapter is None\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        x = self.output(x)\n",
    "        if self.k is None:\n",
    "            # Adjust the output shape for plain networks to make them compatible\n",
    "            # with the rest of the script (loss, metrics, predictions, ...).\n",
    "            # (B, D_OUT) -> (B, 1, D_OUT)\n",
    "            x = x[:, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ed6719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:56:57.714747Z",
     "iopub.status.busy": "2025-03-13T16:56:57.714516Z",
     "iopub.status.idle": "2025-03-13T17:18:20.233598Z",
     "shell.execute_reply": "2025-03-13T17:18:20.232703Z"
    },
    "papermill": {
     "duration": 1282.528366,
     "end_time": "2025-03-13T17:18:20.238803",
     "exception": false,
     "start_time": "2025-03-13T16:56:57.710437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c472fa37a491>:441: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  train_test = train_test.with_columns(pl.Series(name=i, values=pd.factorize(train_test[i])[0]))\n",
      "/usr/local/lib/python3.10/dist-packages/qhoptim/pyt/qhadam.py:133: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  exp_avg.mul_(beta1_adj).add_(1.0 - beta1_adj, d_p)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabular/models/common/layers/soft_trees.py:138: UserWarning: Data-aware initialization is performed on less than 1000 data points. This may cause instability.To avoid potential problems, run this model on a data batch with at least 1000 data samples.You can do so manually before training. Use with torch.no_grad() for memory efficiency.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "┌───────┬────────────┐\n",
      "│ ID    ┆ prediction │\n",
      "│ ---   ┆ ---        │\n",
      "│ i64   ┆ f64        │\n",
      "╞═══════╪════════════╡\n",
      "│ 28800 ┆ -1.255941  │\n",
      "│ 28801 ┆ -0.705521  │\n",
      "│ 28802 ┆ -1.311357  │\n",
      "└───────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "class CIBMTR_Dataset(Dataset):\n",
    "    def __init__(self, num_features, cat_features, efs=None, efs_time=None, y=None, predict=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.efs = efs\n",
    "        self.efs_time = efs_time\n",
    "        self.y = y\n",
    "        self.predict = predict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.num_features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.predict:\n",
    "            return self.num_features[index], self.cat_features[index]\n",
    "        else:\n",
    "            return self.num_features[index], self.cat_features[index], self.efs[index], self.efs_time[index], self.y[index]\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, num_epochs):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.log_train = []\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def prepare_train(self, train):\n",
    "        self.train_dataloader = train\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "\n",
    "    def fit(self, model, train, sched=False):\n",
    "        self.prepare_model(model)\n",
    "        self.prepare_train(train)\n",
    "\n",
    "        self.optim = model.optimizer()\n",
    "        if sched:\n",
    "            self.sched = model.scheduler(self.optim)\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "\n",
    "        global SEED_FIXING\n",
    "\n",
    "        for self.epoch in range(self.num_epochs):\n",
    "            SEED_FIXING += 1\n",
    "            set_seed(SEED_FIXING)\n",
    "\n",
    "            self.fit_epoch()\n",
    "            if sched:\n",
    "                self.sched.step()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.loss(self.model(batch[0].to(DEVICE), batch[1].to(DEVICE)),\n",
    "                                   batch[2].to(DEVICE), batch[3].to(DEVICE), batch[4].to(DEVICE))\n",
    "            self.log_train.append(loss.item())\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "            self.train_batch_idx += 1\n",
    "\n",
    "    def predict(self, data, name):\n",
    "        self.predict_dataloader = data\n",
    "        batches_pred = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if name == 'TabM_Reg1':\n",
    "                for batch in self.predict_dataloader:\n",
    "                    pred = self.model(batch[0].to(DEVICE), batch[1].to(DEVICE)).mean(dim=1)\n",
    "                    batches_pred.append(pred)\n",
    "            if name == 'TabM_Class':\n",
    "                for batch in self.predict_dataloader:\n",
    "                    pred = self.model(batch[0].to(DEVICE), batch[1].to(DEVICE)).mean(dim=1)\n",
    "                    batches_pred.append(pred)\n",
    "            if name == 'MLP_ODST_Class':\n",
    "                for batch in self.predict_dataloader:\n",
    "                    pred = self.model(batch[0].to(DEVICE), batch[1].to(DEVICE))\n",
    "                    batches_pred.append(pred)\n",
    "            preds = torch.cat(batches_pred, dim=0)\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "class TabM_Reg1(nn.Module):\n",
    "    def __init__(self, loss_type, model, lr):\n",
    "        super().__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_nums, x_cats):\n",
    "        x = self.model(x_nums, x_cats).squeeze(-1)\n",
    "        return self.sigmoid(2*x)\n",
    "\n",
    "    def loss(self, pred, efs, efs_time, y):\n",
    "        loss_function_1 = nn.BCELoss()\n",
    "        loss_function_2 = nn.MSELoss()\n",
    "\n",
    "        pred = pred.flatten(0, 1)\n",
    "        y = y.repeat_interleave(K_R)\n",
    "\n",
    "        if self.loss_type == 'BCE':\n",
    "            loss = loss_function_1(pred, y)\n",
    "        if self.loss_type == 'MSE':\n",
    "            loss = loss_function_2(pred, y)\n",
    "        return loss\n",
    "\n",
    "    def optimizer(self):\n",
    "        return QHAdam(make_parameter_groups(self), lr=self.lr)\n",
    "\n",
    "class TabM_Class(nn.Module):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_nums, x_cats):\n",
    "        x = self.model(x_nums, x_cats).squeeze(-1)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def loss(self, pred, efs, efs_time, y):\n",
    "        loss_function = nn.BCELoss()\n",
    "\n",
    "        pred = pred.flatten(0, 1)\n",
    "        efs = efs.repeat_interleave(K_C1)\n",
    "\n",
    "        return loss_function(pred, efs)\n",
    "\n",
    "    def optimizer(self):\n",
    "        return QHAdam(make_parameter_groups(self), lr=self.lr)\n",
    "\n",
    "class MLP_ODST_Class(nn.Module):\n",
    "    def __init__(self, hidden_dim_1, hidden_dim_2, hidden_dim_3, drop_prob, lr):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.embs = torch.nn.ModuleList([nn.Embedding(CAT_SIZE[i], CAT_EMB_SIZE[i]) for i in range(len(CATS))])\n",
    "        self.n_embs = sum(e.embedding_dim for e in self.embs)\n",
    "\n",
    "        self.n_nums = len(NUMS)\n",
    "\n",
    "        self.trees = ODST(self.n_nums + self.n_embs, hidden_dim_1)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.bn_2 = nn.BatchNorm1d(hidden_dim_1)\n",
    "        self.linear_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.act_2 = nn.SiLU()\n",
    "\n",
    "        self.bn_3 = nn.BatchNorm1d(hidden_dim_2)\n",
    "        self.linear_3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
    "        self.act_3 = nn.SiLU()\n",
    "\n",
    "        self.bn_out = nn.BatchNorm1d(hidden_dim_3)\n",
    "        self.out = nn.Linear(hidden_dim_3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_nums, x_cats):\n",
    "        x_embs = [e(x_cats[:, i]) for i, e in enumerate(self.embs)]\n",
    "        x_embs = torch.cat(x_embs, dim=1)\n",
    "\n",
    "        x = torch.cat([x_nums, x_embs], dim=1)\n",
    "\n",
    "        x = self.dropout(self.trees(x))\n",
    "        x = self.act_2(self.linear_2(self.bn_2(x)))\n",
    "        x = self.act_3(self.linear_3(self.bn_3(x)))\n",
    "        return self.sigmoid(self.out(self.bn_out(x))).flatten()\n",
    "\n",
    "    def loss(self, pred, efs, efs_time, y):\n",
    "        loss_function = nn.BCELoss()\n",
    "        return loss_function(pred, efs)\n",
    "\n",
    "    def optimizer(self):\n",
    "        return QHAdam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def scheduler(self, optimizer):\n",
    "        return MultiStepLR(optimizer, milestones=[2,3], gamma=0.1)\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.backends.cudnn.is_available:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_target(df):\n",
    "    df = df.with_columns(pl.Series(name='y', values=df['efs_time']))\n",
    "\n",
    "    df_0 = df.filter(pl.col('efs')==0)\n",
    "    df_1 = df.filter(pl.col('efs')==1)\n",
    "\n",
    "    df_0 = df_0.with_columns(pl.Series(name='y', values=df_0['y'].rank()))\n",
    "    df_0 = df_0.with_columns(pl.Series(name='y', values=df_0['y']/df_0['y'].max()))\n",
    "\n",
    "    df_1 = df_1.with_columns(pl.Series(name='y', values=df_1['y'].rank()))\n",
    "    df_1 = df_1.with_columns(pl.Series(name='y', values=df_1['y']/df_1['y'].max()))\n",
    "\n",
    "    df = pl.concat([df_0, df_1])\n",
    "    df = df.sort(by='ID')\n",
    "\n",
    "    df = df.with_columns(y=pl.when(pl.col('efs')==0).then(COLLAPSE).otherwise(pl.col('y')))\n",
    "\n",
    "    df = df.with_columns(df['y'].cast(pl.Float32))\n",
    "    return df\n",
    "\n",
    "def make_target_by_race(df):\n",
    "    df_0 = df.filter(pl.col('race_group') == 0)\n",
    "    df_0 = make_target(df_0)\n",
    "\n",
    "    df_1 = df.filter(pl.col('race_group') == 1)\n",
    "    df_1 = make_target(df_1)\n",
    "    \n",
    "    df_2 = df.filter(pl.col('race_group') == 2)\n",
    "    df_2 = make_target(df_2)\n",
    "    \n",
    "    df_3 = df.filter(pl.col('race_group') == 3)\n",
    "    df_3 = make_target(df_3)\n",
    "    \n",
    "    df_4 = df.filter(pl.col('race_group') == 4)\n",
    "    df_4 = make_target(df_4)\n",
    "    \n",
    "    df_5 = df.filter(pl.col('race_group') == 5)\n",
    "    df_5 = make_target(df_5)\n",
    "\n",
    "    df = pl.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "    df = df.sort(by='ID')\n",
    "    return df\n",
    "\n",
    "def get_preds(train, test):\n",
    "    preds = np.zeros(test.shape[0])\n",
    "    preds_reg = np.zeros(test.shape[0])\n",
    "    preds_cls_1 = np.zeros(test.shape[0])\n",
    "    preds_cls_2 = np.zeros(test.shape[0])\n",
    "\n",
    "    train = make_target_by_race(train)\n",
    "\n",
    "    train_0 = train.filter(pl.col('efs')==0)\n",
    "    train_1 = train.filter(pl.col('efs')==1)\n",
    "\n",
    "    train_1 = train_1.filter(~pl.col('ID').is_in(outliers_09))\n",
    "\n",
    "    global SEED_FIXING\n",
    "\n",
    "    for repeat_ext in range(NREPEATS_EXTERNAL):\n",
    "        pred_r_repeated = np.zeros(test.shape[0])\n",
    "        pred_c1_repeated = np.zeros(test.shape[0])\n",
    "        pred_c2_repeated = np.zeros(test.shape[0])\n",
    "        for repeat_int in range(NREPEATS_INTERNAL):\n",
    "            SEED_FIXING += 1\n",
    "            set_seed(SEED_FIXING)\n",
    "\n",
    "            TabM_R = Model(\n",
    "                          n_num_features=len(NUMS),\n",
    "                          cat_cardinalities=CAT_SIZE,\n",
    "                          n_classes=None,\n",
    "                          backbone={\n",
    "                              'type': 'MLP',\n",
    "                              'n_blocks': 3,\n",
    "                              'd_block': 768,\n",
    "                              'dropout': 0,\n",
    "                              'activation': 'SiLU',\n",
    "                          },\n",
    "                          bins=rtdl_num_embeddings.compute_bins(train_1.select(NUMS).to_torch(), 48),\n",
    "                          num_embeddings={\n",
    "                              'type': 'PiecewiseLinearEmbeddings',\n",
    "                              'd_embedding': 48,\n",
    "                              'activation': False,\n",
    "                              'version': 'A',\n",
    "                          },\n",
    "                          arch_type='tabm-mini',\n",
    "                          k=K_R,\n",
    "                          )\n",
    "\n",
    "            ds_train_r = CIBMTR_Dataset(num_features=train_1.select(NUMS).to_torch(),\n",
    "                                        cat_features=train_1.select(CATS).to_torch(),\n",
    "                                        efs=train_1['efs'].to_torch(),\n",
    "                                        efs_time=train_1['efs_time'].to_torch(),\n",
    "                                        y=train_1['y'].to_torch(),\n",
    "                                        predict=False)\n",
    "            dl_train_r = DataLoader(ds_train_r, batch_size=BS_R, shuffle=True, drop_last=True)\n",
    "\n",
    "            ds_predict_r = CIBMTR_Dataset(num_features=test.select(NUMS).to_torch(),\n",
    "                                          cat_features=test.select(CATS).to_torch())\n",
    "            dl_predict_r = DataLoader(ds_predict_r, batch_size=BS_R, shuffle=False, drop_last=False)\n",
    "\n",
    "            model_r = TabM_Reg1(loss_type='BCE', model=TabM_R, lr=LR_R).to(DEVICE)\n",
    "\n",
    "            trainer_r = Trainer(num_epochs=E_R)\n",
    "            trainer_r.fit(model_r, dl_train_r, sched=False)\n",
    "\n",
    "            pred_r = trainer_r.predict(dl_predict_r, 'TabM_Reg1')\n",
    "            pred_r_repeated += pred_r\n",
    "\n",
    "            SEED_FIXING += 1\n",
    "            set_seed(SEED_FIXING)\n",
    "\n",
    "            TabM_C = Model(\n",
    "                          n_num_features=len(NUMS),\n",
    "                          cat_cardinalities=CAT_SIZE,\n",
    "                          n_classes=None,\n",
    "                          backbone={\n",
    "                              'type': 'MLP',\n",
    "                              'n_blocks': 2,\n",
    "                              'd_block': 512,\n",
    "                              'dropout': 0,\n",
    "                              'activation': 'SiLU',\n",
    "                          },\n",
    "                          bins=rtdl_num_embeddings.compute_bins(train.select(NUMS).to_torch(), 48),\n",
    "                          num_embeddings={\n",
    "                              'type': 'PiecewiseLinearEmbeddings',\n",
    "                              'd_embedding': 48,\n",
    "                              'activation': False,\n",
    "                              'version': 'B',\n",
    "                          },\n",
    "                          arch_type='tabm-mini',\n",
    "                          k=K_C1,\n",
    "                          )\n",
    "\n",
    "            ds_train_c1 = CIBMTR_Dataset(num_features=train.select(NUMS).to_torch(),\n",
    "                                         cat_features=train.select(CATS).to_torch(),\n",
    "                                         efs=train['efs'].to_torch(),\n",
    "                                         efs_time=train['efs_time'].to_torch(),\n",
    "                                         y=train['y'].to_torch(),\n",
    "                                         predict=False)\n",
    "            dl_train_c1 = DataLoader(ds_train_c1, batch_size=BS_C1, shuffle=True, drop_last=True)\n",
    "\n",
    "            ds_predict_c1 = CIBMTR_Dataset(num_features=test.select(NUMS).to_torch(),\n",
    "                                           cat_features=test.select(CATS).to_torch())\n",
    "            dl_predict_c1 = DataLoader(ds_predict_c1, batch_size=BS_C1, shuffle=False, drop_last=False)\n",
    "\n",
    "            model_c1 = TabM_Class(model=TabM_C, lr=LR_C1).to(DEVICE)\n",
    "\n",
    "            trainer_c1 = Trainer(num_epochs=E_C1)\n",
    "            trainer_c1.fit(model_c1, dl_train_c1, sched=False)\n",
    "\n",
    "            pred_c1 = trainer_c1.predict(dl_predict_c1, 'TabM_Class')\n",
    "            pred_c1_repeated += pred_c1\n",
    "\n",
    "            SEED_FIXING += 1\n",
    "            set_seed(SEED_FIXING)\n",
    "\n",
    "            ds_train_c2 = CIBMTR_Dataset(num_features=train.select(NUMS).to_torch(),\n",
    "                                         cat_features=train.select(CATS).to_torch(),\n",
    "                                         efs=train['efs'].to_torch(),\n",
    "                                         efs_time=train['efs_time'].to_torch(),\n",
    "                                         y=train['y'].to_torch(),\n",
    "                                         predict=False)\n",
    "            dl_train_c2 = DataLoader(ds_train_c2, batch_size=BS_C2, shuffle=True, drop_last=True)\n",
    "\n",
    "            ds_predict_c2 = CIBMTR_Dataset(num_features=test.select(NUMS).to_torch(),\n",
    "                                           cat_features=test.select(CATS).to_torch())\n",
    "            dl_predict_c2 = DataLoader(ds_predict_c2, batch_size=BS_C2, shuffle=False, drop_last=False)\n",
    "\n",
    "            model_c2 = MLP_ODST_Class(hidden_dim_1=HD_1_C2, hidden_dim_2=HD_2_C2, hidden_dim_3=HD_3_C2, drop_prob=DP_C2, lr=LR_C2).to(DEVICE)\n",
    "\n",
    "            trainer_c2 = Trainer(num_epochs=E_C2)\n",
    "            trainer_c2.fit(model_c2, dl_train_c2, sched=True)\n",
    "\n",
    "            pred_c2 = trainer_c2.predict(dl_predict_c2, 'MLP_ODST_Class')\n",
    "            pred_c2_repeated += pred_c2\n",
    "\n",
    "        pred_r_repeated /= NREPEATS_INTERNAL\n",
    "        pred_c1_repeated /= NREPEATS_INTERNAL\n",
    "        pred_c2_repeated /= NREPEATS_INTERNAL\n",
    "        pred_repeated = COLLAPSE * (1 - pred_c1_repeated) + pred_r_repeated * pred_c1_repeated\n",
    "\n",
    "        preds += pred_repeated\n",
    "        preds_reg += pred_r_repeated\n",
    "        preds_cls_1 += pred_c1_repeated\n",
    "        preds_cls_2 += pred_c2_repeated\n",
    "\n",
    "    preds /= NREPEATS_EXTERNAL\n",
    "    preds_reg /= NREPEATS_EXTERNAL\n",
    "    preds_cls_1 /= NREPEATS_EXTERNAL\n",
    "    preds_cls_2 /= NREPEATS_EXTERNAL\n",
    "    return preds, preds_reg, preds_cls_1, preds_cls_2\n",
    "\n",
    "train = pl.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "train = train.with_columns(pl.Series(name='donor_age_is_null', values=train['donor_age'].is_null().cast(pl.Int64)))\n",
    "\n",
    "test = pl.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "test = test.with_columns(pl.Series(name='donor_age_is_null', values=test['donor_age'].is_null().cast(pl.Int64)))\n",
    "\n",
    "outliers_09 = np.load('/kaggle/input/cibmtr-competition/outliers_0.9.npy')\n",
    "\n",
    "\n",
    "IDS = ['ID']\n",
    "TARGETS = ['efs', 'efs_time']\n",
    "FEATURES = []\n",
    "\n",
    "NUMS = ['donor_age', 'age_at_hct']\n",
    "FEATURES.extend(NUMS)\n",
    "\n",
    "CATS = [feat for feat in train.columns if not (feat in IDS or feat in TARGETS or feat in NUMS)]\n",
    "FEATURES.extend(CATS)\n",
    "\n",
    "for i in TARGETS:\n",
    "    if train[i].dtype.is_integer():\n",
    "        train = train.with_columns(train[i].cast(pl.Int32))\n",
    "    if train[i].dtype.is_float():\n",
    "        train = train.with_columns(train[i].cast(pl.Float32))\n",
    "\n",
    "train_targets = train[TARGETS]\n",
    "train = train.drop(TARGETS)\n",
    "\n",
    "train_test = pl.concat([train, test])\n",
    "\n",
    "for i in NUMS:\n",
    "    if train_test[i].dtype.is_integer():\n",
    "        train_test = train_test.with_columns(train_test[i].cast(pl.Int32))\n",
    "    if train_test[i].dtype.is_float():\n",
    "        train_test = train_test.with_columns(train_test[i].cast(pl.Float32))\n",
    "\n",
    "for i in CATS:\n",
    "    if train_test[i].dtype.is_numeric():\n",
    "        train_test = train_test.with_columns(train_test[i].cast(pl.String))\n",
    "\n",
    "CAT_SIZE = []\n",
    "CAT_EMB_SIZE = []\n",
    "\n",
    "for i in NUMS:\n",
    "    train_test = train_test.with_columns(pl.Series(name=i, values=(train_test[i]-train_test[i].mean())/train_test[i].std()))\n",
    "    train_test = train_test.with_columns(pl.Series(name=i, values=train_test[i].fill_null(strategy='zero')))\n",
    "\n",
    "for i in CATS:\n",
    "    train_test = train_test.with_columns(pl.Series(name=i, values=pd.factorize(train_test[i])[0]))\n",
    "    train_test = train_test.with_columns(pl.Series(name=i, values=train_test[i]-train_test[i].min()))\n",
    "    train_test = train_test.with_columns(train_test[i].cast(pl.Int64))\n",
    "    CAT_SIZE.append(train_test[i].n_unique())\n",
    "    CAT_EMB_SIZE.append(8)\n",
    "\n",
    "train = train_test[:len(train)]\n",
    "train = train.with_columns(train_targets)\n",
    "\n",
    "test = train_test[len(train):]\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "COLLAPSE = 1.35\n",
    "\n",
    "NREPEATS_INTERNAL = 5\n",
    "NREPEATS_EXTERNAL = 5\n",
    "\n",
    "E_R = 30\n",
    "BS_R = 512\n",
    "LR_R = 0.005\n",
    "K_R = 48\n",
    "\n",
    "E_C1 = 9\n",
    "BS_C1 = 768\n",
    "LR_C1 = 0.0035\n",
    "K_C1 = 28\n",
    "\n",
    "E_C2 = 4\n",
    "BS_C2 = 256\n",
    "LR_C2 = 0.05\n",
    "HD_1_C2 = 512\n",
    "HD_2_C2 = 256\n",
    "HD_3_C2 = 128\n",
    "DP_C2 = 0\n",
    "\n",
    "SEED_FIXING = 0\n",
    "\n",
    "preds, preds_reg, preds_cls_1, preds_cls_2 = get_preds(train, test)\n",
    "preds = -preds\n",
    "\n",
    "np.save('tabm_ones_pred.npy', preds_reg)\n",
    "np.save('tabm_clf_pred.npy', preds_cls_1)\n",
    "np.save('mlp_clf_pred.npy', preds_cls_2)\n",
    "\n",
    "submission = pl.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\n",
    "submission = submission.with_columns(pl.Series(name='prediction', values=preds))\n",
    "submission.write_csv('submission_nn.csv')\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de2b44",
   "metadata": {
    "papermill": {
     "duration": 0.003135,
     "end_time": "2025-03-13T17:18:20.245440",
     "exception": false,
     "start_time": "2025-03-13T17:18:20.242305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc11989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T17:18:20.252959Z",
     "iopub.status.busy": "2025-03-13T17:18:20.252725Z",
     "iopub.status.idle": "2025-03-13T17:20:10.192371Z",
     "shell.execute_reply": "2025-03-13T17:20:10.191196Z"
    },
    "papermill": {
     "duration": 109.94533,
     "end_time": "2025-03-13T17:20:10.194163",
     "exception": false,
     "start_time": "2025-03-13T17:18:20.248833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cibmtr-scripts/predict_catboost_ones_denoized_inc10.py:130: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\r\n",
      "  df = df.with_columns(pl.Series(name=col, values=pd.Series(pd.factorize(df[col])[0])))\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  4.13it/s]\r\n",
      "Finished ones prediction.\r\n",
      "/kaggle/input/cibmtr-scripts/predict_catboost_zeroes.py:129: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\r\n",
      "  df = df.with_columns(pl.Series(name=col, values=pd.Series(pd.factorize(df[col])[0])))\r\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 109.20it/s]\r\n",
      "Finished zeroes prediction.\r\n",
      "/kaggle/input/cibmtr-scripts/predict_catboost_clf_inc10.py:132: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\r\n",
      "  df = df.with_columns(pl.Series(name=col, values=pd.Series(pd.factorize(df[col])[0])))\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 39.00it/s]\r\n",
      "Finished clf prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.20it/s]\r\n",
      "Finished LGB ones prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.50it/s]\r\n",
      "Finished LGB zeroes prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 12.76it/s]\r\n",
      "Finished LGB clf prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.21it/s]\r\n",
      "Finished XGB ones prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.72it/s]\r\n",
      "Finished XGB zeroes prediction.\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.80it/s]\r\n",
      "Finished XGB clf prediction.\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/input/cibmtr-scripts/predict_catboost_ones_denoized_inc10.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_catboost_zeroes.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_catboost_clf_inc10.py\n",
    "\n",
    "!python /kaggle/input/cibmtr-scripts/predict_lgb_ones_denoized_inc10.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_lgb_zeroes.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_lgb_clf_inc10.py\n",
    "\n",
    "!python /kaggle/input/cibmtr-scripts/predict_xgb_ones_denoized_inc10.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_xgb_zeroes.py\n",
    "!python /kaggle/input/cibmtr-scripts/predict_xgb_clf_inc10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab2539",
   "metadata": {
    "papermill": {
     "duration": 0.017623,
     "end_time": "2025-03-13T17:20:10.230436",
     "exception": false,
     "start_time": "2025-03-13T17:20:10.212813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae4f0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T17:20:10.266464Z",
     "iopub.status.busy": "2025-03-13T17:20:10.266025Z",
     "iopub.status.idle": "2025-03-13T17:20:10.287734Z",
     "shell.execute_reply": "2025-03-13T17:20:10.286958Z"
    },
    "papermill": {
     "duration": 0.041202,
     "end_time": "2025-03-13T17:20:10.288894",
     "exception": false,
     "start_time": "2025-03-13T17:20:10.247692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30101951 1.30311279 1.29566588]\n",
      "[0.72987528 0.2582193  0.45997082]\n",
      "[0.15037933 0.60612552 0.04380817]\n",
      "[1.30067287 1.30305424 1.29571412]\n",
      "[0.71412316 0.23153858 0.60182431]\n",
      "[0.16305168 0.63119842 0.05150985]\n",
      "[1.3008629 1.3028618 1.2956567]\n",
      "[0.68303436 0.233275   0.5940789 ]\n",
      "[0.16296439 0.6380571  0.03650875]\n",
      "[0.71091613 0.28753632 0.48195353]\n",
      "[0.14715654 0.60658601 0.04451545]\n",
      "[0.12475702 0.56829658 0.02938798]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "DATA_DIR = '/kaggle/input/equity-post-HCT-survival-predictions'\n",
    "\n",
    "# zeroes parameters\n",
    "ZEROES_SHIFT = 0.869\n",
    "LGB_ZEROES_COEF = 0.333\n",
    "CATBOOST_ZEROES_COEF = 0.333\n",
    "XGB_ZEROES_COEF = 0.333\n",
    "\n",
    "# ones parameters\n",
    "LGB_ONES_COEF = 0.4131\n",
    "CATBOOST_ONES_COEF = 0.1409\n",
    "XGB_ONES_COEF = 0.3376\n",
    "TABM_ONES_COEF = 0.1084\n",
    "\n",
    "# clf parameters\n",
    "CLF_SHIFT = -2.1958\n",
    "LGB_CLF_COEF = 1.525\n",
    "CATBOOST_CLF_COEF = 2.069\n",
    "XGB_CLF_COEF = 0.4004\n",
    "MLP_CLF_COEF = 1.5452\n",
    "TABM_CLF_COEF = 1.8006\n",
    "\n",
    "# load predictions\n",
    "lgb_zeroes_pred = np.load('lgb_test_pred_zeroes.npy');print(lgb_zeroes_pred)\n",
    "lgb_ones_pred = np.load('lgb_test_pred_ones.npy');print(lgb_ones_pred)\n",
    "lgb_clf_pred = np.load('lgb_test_pred_clf.npy');print(lgb_clf_pred)\n",
    "\n",
    "catboost_zeroes_pred = np.load('catboost_test_pred_zeroes.npy');print(catboost_zeroes_pred)\n",
    "catboost_ones_pred = np.load('catboost_test_pred_ones.npy');print(catboost_ones_pred)\n",
    "catboost_clf_pred = np.load('catboost_test_pred_clf.npy');print(catboost_clf_pred)\n",
    "\n",
    "xgb_zeroes_pred = np.load('xgb_test_pred_zeroes.npy');print(xgb_zeroes_pred)\n",
    "xgb_ones_pred = np.load('xgb_test_pred_ones.npy');print(xgb_ones_pred)\n",
    "xgb_clf_pred = np.load('xgb_test_pred_clf.npy');print(xgb_clf_pred)\n",
    "\n",
    "tabm_ones_pred = np.load('tabm_ones_pred.npy');print(tabm_ones_pred)\n",
    "tabm_clf_pred = np.load('tabm_clf_pred.npy');print(tabm_clf_pred)\n",
    "mlp_clf_pred = np.load('mlp_clf_pred.npy');print(mlp_clf_pred)\n",
    "\n",
    "# blend zeroes\n",
    "test_pred_zeroes = ZEROES_SHIFT + LGB_ZEROES_COEF*lgb_zeroes_pred + CATBOOST_ZEROES_COEF*catboost_zeroes_pred + XGB_ZEROES_COEF*xgb_zeroes_pred\n",
    "\n",
    "# blend ones\n",
    "test_pred_ones = LGB_ONES_COEF*lgb_ones_pred + CATBOOST_ONES_COEF*catboost_ones_pred + XGB_ONES_COEF*xgb_ones_pred + TABM_ONES_COEF*tabm_ones_pred \n",
    "\n",
    "# blend clfs\n",
    "test_pred_clf = LGB_CLF_COEF*lgb_clf_pred + CATBOOST_CLF_COEF*catboost_clf_pred + XGB_CLF_COEF*xgb_clf_pred + MLP_CLF_COEF*mlp_clf_pred + TABM_CLF_COEF*tabm_clf_pred\n",
    "test_pred_clf = expit(test_pred_clf + CLF_SHIFT)\n",
    "\n",
    "# assemble submission\n",
    "ss = pl.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "blend_prediction = test_pred_clf*test_pred_ones + (1-test_pred_clf)*test_pred_zeroes\n",
    "ss = ss.with_columns(prediction=-blend_prediction)\n",
    "ss.write_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4eb414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T17:20:10.324907Z",
     "iopub.status.busy": "2025-03-13T17:20:10.324634Z",
     "iopub.status.idle": "2025-03-13T17:20:10.528042Z",
     "shell.execute_reply": "2025-03-13T17:20:10.527141Z"
    },
    "papermill": {
     "duration": 0.223342,
     "end_time": "2025-03-13T17:20:10.529772",
     "exception": false,
     "start_time": "2025-03-13T17:20:10.306430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,prediction\r\n",
      "28800,-1.805910178151666\r\n",
      "28801,-0.4307264899263298\r\n",
      "28802,-1.9472455279143077\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e67c3",
   "metadata": {
    "papermill": {
     "duration": 0.017553,
     "end_time": "2025-03-13T17:20:10.565606",
     "exception": false,
     "start_time": "2025-03-13T17:20:10.548053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "datasetId": 6727025,
     "sourceId": 10883680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6762885,
     "sourceId": 10918363,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6714612,
     "sourceId": 10933467,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6790313,
     "sourceId": 10933543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6728120,
     "sourceId": 10933573,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1445.378463,
   "end_time": "2025-03-13T17:20:13.631269",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-13T16:56:08.252806",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
